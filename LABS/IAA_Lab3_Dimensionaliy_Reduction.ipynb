{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAA Laboratori 3 - Compressing Data via Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (5.18.0)\n",
      "Requirement already satisfied: numpy==1.25 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.25.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (5.9.2)\n",
      "Requirement already satisfied: umap-learn in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.5.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from plotly) (23.2)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nbformat) (2.19.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nbformat) (4.20.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nbformat) (5.5.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nbformat) (5.14.0)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from umap-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from umap-learn) (1.3.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from umap-learn) (0.58.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from umap-learn) (0.5.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from umap-learn) (4.66.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema>=2.6->nbformat) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema>=2.6->nbformat) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema>=2.6->nbformat) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema>=2.6->nbformat) (0.15.2)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jupyter-core->nbformat) (4.0.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jupyter-core->nbformat) (306)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm->umap-learn) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: C:\\Users\\Usuario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly numpy==1.25 nbformat umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Maximum Variance Formulation](#Maximum-Variance-Formulation)\n",
    "  - [scikit-learn Comparisson](#Sklearn-Comparisson)\n",
    "- [Unsupervised dimensionality reduction via principal component analysis](#Unsupervised-dimensionality-reduction-via-principal-component-analysis-128)\n",
    "  - [The main steps behind principal component analysis](#The-main-steps-behind-principal-component-analysis)\n",
    "  - [Extracting the principal components step-by-step](#Extracting-the-principal-components-step-by-step)\n",
    "  - [Total and explained variance](#Total-and-explained-variance)\n",
    "  - [Feature transformation](#Feature-transformation)\n",
    "  - [Principal component analysis in scikit-learn](#Principal-component-analysis-in-scikit-learn)\n",
    "- [Supervised data compression via linear discriminant analysis](#Supervised-data-compression-via-linear-discriminant-analysis)\n",
    "  - [Principal component analysis versus linear discriminant analysis](#Principal-component-analysis-versus-linear-discriminant-analysis)\n",
    "  - [LDA via scikit-learn](#LDA-via-scikit-learn)\n",
    "- [Projections with Multi-dimensional Scaling (MDS)](#Projections-with-Multi-dimensional-Scaling-(MDS))\n",
    "- [Projections with Uniform Manifold Approximation and Projection (UMAP)](#Projections-with-Uniform-Manifold-Approximation-and-Projection-(UMAP))\n",
    "- [Projections with t-distributed Stochastic Neighbor Embedding (t-SNE)](#Projections-with-t-distributed-Stochastic-Neighbor-Embedding-(t-SNE)) \n",
    "- [Comparison of Manifold Learning methods](#Comparison-of-Manifold-Learning-methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eig\n",
    "from numpy.random import randn, rand, seed\n",
    "from sklearn.datasets import load_iris\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "np.set_printoptions(precision=8, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Variance Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal D = \\{{\\bf x}_n | {\\bf x} \\in \\mathbb{R}^D\\}_{n=1}^N$ be a dataset. We seek to **maximize the variance of the projected data**.\n",
    "\n",
    "Consider the following optimization problem \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\max_{{\\bf u}_1} &\\ {{\\bf u}_1^T{\\bf S}{\\bf u}_1} \\\\\n",
    "    \\text{s.t.} &\\ {\\bf u}_1^T{\\bf u}_1 = 1\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "Where\n",
    "$$\n",
    "    {\\bf S} = \\frac{1}{N}\\sum_{n=1}^N\\big({\\bf x}_n - \\bar{\\bf x}\\big)\\big({\\bf x}_n - \\bar{\\bf x}\\big)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(314)\n",
    "nsamples = 20\n",
    "x1 = np.linspace(0, 5, nsamples) + rand(nsamples) * 0.5\n",
    "x2 = 3 * x1 + 5 + randn(nsamples) * 2\n",
    "X = np.c_[x1, x2]\n",
    "X_bar = X.mean(axis=0, keepdims=True)\n",
    "X = X - X_bar\n",
    "plt.scatter(*X.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.einsum(\"ij,ik->jk\", X, X) / nsamples\n",
    "eigvals, eigvects = eig(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = eigvals.argmax()\n",
    "u1 = eigvects[:, [l]]\n",
    "\n",
    "X_proj = X @ u1\n",
    "\n",
    "plt.scatter(X_proj, np.ones(nsamples), s=10)\n",
    "plt.title(\"Projected Data\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn Comparisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "np.c_[X_proj, pca.transform(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposition\n",
    "\n",
    "The linear projection onto an $M$-dimensional subspace that maximizes the variance of the projected data is defined by the $M$ eigenvectors of the data covariance matrix $S$, corresponding to the $M$ largest eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = eigvals.argmax()\n",
    "l2 = eigvals.argmin()\n",
    "\n",
    "u1 = eigvects[:, [l1]]\n",
    "u2 = eigvects[:, [l2]]\n",
    "\n",
    "u1.T @ u2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example: The iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris[\"data\"], iris[\"target\"]\n",
    "nsamples = len(X)\n",
    "\n",
    "S = np.einsum(\"ij,ik->jk\", X, X) / nsamples\n",
    "eigvals, eigvects = eig(S)\n",
    "\n",
    "# Sort in descending order\n",
    "l = (-eigvals).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = eigvects[:, l[:2]]\n",
    "X_proj = X @ u\n",
    "\n",
    "plt.scatter(*X_proj.T, c=y, cmap=\"Set1\")\n",
    "plt.title(\"Principal Subspace\", fontsize=15)\n",
    "plt.xlabel(r\"${\\bf u}_1$\", fontsize=15)\n",
    "plt.ylabel(r\"${\\bf u}_2$\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised dimensionality reduction via principal component analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main steps behind principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and return the digits dataset (classification).\n",
    "\n",
    "Each datapoint is a 8x8 image of a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=3)\n",
    "X, y = digits.data, digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow version\n",
    "\n",
    "Each datapoint is a 28x28 image of a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "#data = mnist.load_data()\n",
    "#train, test = data\n",
    "#Xtrain, ytrain = train\n",
    "#Xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "PLAYTIME: Load the full MNIST dataset and repeat the analysis with full dataset\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "_ = fig.suptitle(\"A selection from the 64-dimensional (8x8) digits dataset\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X[y == 0]\n",
    "X_sample = X_sample.reshape(-1,8,8)\n",
    "N, M, M = X_sample.shape\n",
    "\n",
    "Xbar = X_sample.mean(axis=0)\n",
    "Xhat = (X_sample - Xbar).reshape(N, -1)\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "S = np.einsum(\"ij,ik->jk\", Xhat, Xhat) / N\n",
    "eigvals, eigvects = eig(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because each eigenvector of the covariance matrix is a vector in the original $D$-dimensional space, we can represent the eigenvectors as images of the same size as data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_eigval(eigval):\n",
    "    label = rf\"$\\lambda={eigval:.2e}\"\n",
    "    label = label.replace(\"e+0\", \"\\cdot 10^{\")\n",
    "    return label + \"}$\"\n",
    "\n",
    "npc = 4 # number of principal components\n",
    "fig, ax = plt.subplots(1, npc + 1, figsize=(15, 4))\n",
    "\n",
    "\n",
    "principal_subspace = eigvects[:, :npc].reshape(M, M , npc).real\n",
    "eigvectors = [format_eigval(li) for li in eigvals[:npc].real]\n",
    "labels = [\"Mean\"] + eigvectors\n",
    "pca_arrays = np.concatenate((Xbar[..., np.newaxis], principal_subspace), axis=-1)\n",
    "\n",
    "for n, label in enumerate(labels):\n",
    "    ax[n].imshow(pca_arrays[..., n])\n",
    "    ax[n].axis(\"off\")\n",
    "    ax[n].set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The PCA approximation to a data vector ${\\bf x}_n$ is given by**\n",
    "\n",
    "$$\n",
    "    \\tilde{\\bf x}_n = \\bar{\\bf x} + \\sum_{m=1}^M\\left({\\bf x}_n^{\\text T}{\\bf u}_m - \\bar{\\bf x}^{\\text T}{\\bf u}_m\\right) {\\bf u}_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Xhat[[13]].reshape(M, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = X.reshape(-1,8,8)\n",
    "N, M, M = X.shape\n",
    "Xbar = X.mean(axis=0)\n",
    "Xhat = (X - Xbar).reshape(N, -1)\n",
    "\n",
    "\n",
    "S = np.einsum(\"ij,ik->jk\", Xhat, Xhat) / N\n",
    "eigvals, eigvects = eig(S)\n",
    "\n",
    "u = eigvects[:, :2]\n",
    "proj = Xhat @ u\n",
    "plt.scatter(*proj.T, c=y, alpha=0.8, cmap=\"Dark2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the principal components step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=3)\n",
    "X, y = digits.data, digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, \n",
    "                     stratify=y,\n",
    "                     random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('\\nEigenvalues \\n%s' % eigen_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(1, 65), var_exp, alpha=0.5, align='center',\n",
    "        label='Individual explained variance')\n",
    "plt.step(range(1, 65), cum_var_exp, where='mid',\n",
    "         label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_02.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "               for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Depending on which version of NumPy and LAPACK you are using, you may obtain the Matrix W with its signs flipped. Please note that this is not an issue: If $v$ is an eigenvector of a matrix $\\Sigma$, we have\n",
    "\n",
    "$$\\Sigma v = \\lambda v,$$\n",
    "\n",
    "where $\\lambda$ is our eigenvalue,\n",
    "\n",
    "\n",
    "then $-v$ is also an eigenvector that has the same eigenvalue, since\n",
    "$$\\Sigma \\cdot (-v) = -\\Sigma v = -\\lambda v = \\lambda \\cdot (-v).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std[0].dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = X_train_std.dot(w)\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train == l, 0], \n",
    "                X_train_pca[y_train == l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_03.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "The following four code cells illustrate how scikit-learn replicates the results from our own PCA implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, 65), pca.explained_variance_ratio_, alpha=0.5, align='center')\n",
    "plt.step(range(1, 65), np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1])\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot examples by class\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.6, \n",
    "                    color=cmap(idx),\n",
    "                    edgecolor='black',\n",
    "                    marker=markers[idx], \n",
    "                    label=cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logistic regression classifier using the first 2 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')\n",
    "lr = lr.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_04.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_05.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised data compression via linear discriminant analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis versus linear discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the generalized eigenvalue problem for the matrix $S_W^{-1}S_B$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA via scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')\n",
    "lr = lr.fit(X_train_lda, y_train)\n",
    "\n",
    "plot_decision_regions(X_train_lda, y_train, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_09.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lda = lda.transform(X_test_std)\n",
    "\n",
    "plot_decision_regions(X_test_lda, y_test, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_10.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections with Multi-dimensional Scaling (MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "\n",
    "In general, MDS is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.\n",
    "\n",
    "There exists two types of MDS algorithm: metric and non metric. In scikit-learn, the class MDS implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.\n",
    "\n",
    "MDS is not only an effective technique for dimensionality reduction but also for data visualization. It maintains the same clusters and patterns of high-dimensional data in the lower-dimensional space so you can boil down, say, a 5-dimensional dataset to a 3-dimensional dataset which you can interpret much more easily and naturally.\n",
    "\n",
    "Normally the distance measure used in MDS is the Euclidean distance, however, any other suitable dissimilarity metric can be used when applying MDS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=3)\n",
    "\n",
    "mds = MDS(n_components=2, n_init=1, max_iter=120, n_jobs=2, normalized_stress=\"auto\")\n",
    "projections = mds.fit_transform(digits.data)\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=digits.target.astype(str), labels={'color': 'digit'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = mds.stress_\n",
    "print(stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "dist_manhattan = manhattan_distances(digits.data)\n",
    "mds = MDS(n_components=2, n_init=1, max_iter=120, n_jobs=2, normalized_stress=\"auto\",dissimilarity='precomputed')\n",
    "# Get the embeddings\n",
    "projections = mds.fit_transform(dist_manhattan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = mds.stress_\n",
    "print(stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=digits.target.astype(str), labels={'color': 'digit'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "PLAYTIME: Write a function for selecting the number of MDS projection dimensions based on the stress value  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections with t-distributed Stochastic Neighbor Embedding (T-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE (TSNE) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student’s t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:\n",
    "\n",
    "- Revealing the structure at many scales on a single map\n",
    "- Revealing data that lie in multiple, different, manifolds or clusters\n",
    "- Reducing the tendency to crowd points together at the center\n",
    "\n",
    "While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.\n",
    "\n",
    "The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.\n",
    "\n",
    "The disadvantages to using t-SNE are roughly:\n",
    "- t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes\n",
    "- The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.\n",
    "- The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.\n",
    "- Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using init='pca').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "projections = tsne.fit_transform(digits.data)\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=digits.target.astype(str), labels={'color': 'digit'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, random_state=0)\n",
    "\n",
    "proj_3d = tsne.fit_transform(digits.data)\n",
    "\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "     color=digits.target.astype(str), labels={'color': 'digit'}\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig_3d.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "PLAYTIME: Study the values of perplexity and learning rate. How it affects the KL divergence value? How it affects the projection?\n",
    "Write a function for repeating the t-SNE projection with different perplexity and learning rate values and select the best projection based on the KL divergence value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections with Uniform Manifold Approximation and Projection (UMAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a popular dimensionality reduction algorithm that arises from probability theory. Simply put, it projects the high-dimensional data points (sometimes with hundreds of features) into 2D/3D by inducing the projected data to have a similar distribution as the original data points by minimizing something called the KL divergence.\n",
    "\n",
    "Compared to a method like Principal Component Analysis (PCA), it takes significantly more time to converge, but present significantly better insights when visualized. For example, by projecting features of a flowers, it will be able to distinctly group\n",
    "\n",
    "Just like t-SNE, UMAP is a dimensionality reduction specifically designed for visualizing complex data in low dimensions (2D or 3D). As the number of data points increase, UMAP becomes more time efficient compared to TSNE.\n",
    "\n",
    "In the example below, we see how easy it is to use UMAP as a drop-in replacement for scikit-learn's manifold.TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.datasets import load_digits\n",
    "from umap import UMAP\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "umap_2d = UMAP(random_state=0)\n",
    "umap_2d.fit(digits.data)\n",
    "\n",
    "projections = umap_2d.transform(digits.data)\n",
    "\n",
    "fig = px.scatter(\n",
    "    projections, x=0, y=1,\n",
    "    color=digits.target.astype(str), labels={'color': 'digit'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "umap_3d.fit(digits.data)\n",
    "proj_3d = umap_3d.transform(digits.data)\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "     color=digits.target.astype(str), labels={'color': 'digit'}\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig_3d.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "PLAYTIME: Study the values of n_neighbors and min_dist parameters for UMAP. How it affects the projection?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Manifold Learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=3)\n",
    "X, y = digits.data, digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import offsetbox\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def plot_embedding(X, title):\n",
    "    _, ax = plt.subplots()\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    for digit in digits.target_names:\n",
    "        ax.scatter(\n",
    "            *X[y == digit].T,\n",
    "            marker=f\"${digit}$\",\n",
    "            s=60,\n",
    "            color=plt.cm.Dark2(digit),\n",
    "            alpha=0.425,\n",
    "            zorder=2,\n",
    "        )\n",
    "    shown_images = np.array([[1.0, 1.0]])  # just something big\n",
    "    for i in range(X.shape[0]):\n",
    "        # plot every digit on the embedding\n",
    "        # show an annotation box for a group of digits\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            # don't show points that are too close\n",
    "            continue\n",
    "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "        imagebox = offsetbox.AnnotationBbox(\n",
    "            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]\n",
    "        )\n",
    "        imagebox.set(zorder=1)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import (\n",
    "    MDS,\n",
    "    TSNE,\n",
    "    Isomap,\n",
    ")\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "embeddings = {\n",
    "    \"Random projection embedding\": SparseRandomProjection(\n",
    "        n_components=2, random_state=42\n",
    "    ),\n",
    "    \"Truncated SVD embedding\": TruncatedSVD(n_components=2),\n",
    "    \"Linear Discriminant Analysis embedding\": LinearDiscriminantAnalysis(\n",
    "        n_components=2\n",
    "    ),\n",
    "    \"Isomap embedding\": Isomap(n_neighbors=n_neighbors, n_components=2),\n",
    "    \"MDS embedding\": MDS(\n",
    "        n_components=2, n_init=1, max_iter=120, n_jobs=2, normalized_stress=\"auto\"\n",
    "    ),\n",
    "    \"t-SNE embeedding\": TSNE(\n",
    "        n_components=2,\n",
    "        n_iter=500,\n",
    "        n_iter_without_progress=150,\n",
    "        n_jobs=2,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    \"NCA embedding\": NeighborhoodComponentsAnalysis(\n",
    "        n_components=2, init=\"pca\", random_state=0\n",
    "    ),\n",
    "    \"UMAP embedding\": UMAP(n_components=2, random_state=42),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LinearDiscriminantAnalysis and the NeighborhoodComponentsAnalysis, are supervised dimensionality reduction method, i.e. they make use of the provided labels, contrary to other methods.\n",
    "\n",
    "The TSNE is initialized with the embedding that is generated by PCA in this example. It ensures global stability of the embedding, i.e., the embedding does not depend on random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "projections, timing = {}, {}\n",
    "for name, transformer in embeddings.items():\n",
    "    if name.startswith(\"Linear Discriminant Analysis\"):\n",
    "        data = X.copy()\n",
    "        data.flat[:: X.shape[1] + 1] += 0.01  # Make X invertible\n",
    "    else:\n",
    "        data = X\n",
    "\n",
    "    print(f\"Computing {name}...\")\n",
    "    start_time = time()\n",
    "    projections[name] = transformer.fit_transform(data, y)\n",
    "    timing[name] = time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in timing:\n",
    "    title = f\"{name} (time {timing[name]:.3f}s)\"\n",
    "    plot_embedding(projections[name], title)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
